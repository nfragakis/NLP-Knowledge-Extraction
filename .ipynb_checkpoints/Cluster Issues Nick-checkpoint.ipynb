{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import spacy\n",
    "import ast\n",
    "import re\n",
    "import nltk\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues_df = pd.read_csv('6_21_validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thresholding Impact\n",
    "- 80% for all relations\n",
    "    - 31% w no issues detected (776 / 2451)\n",
    "- 90% for all relations\n",
    "    - 34% w no issues detected (826 / 2451)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data import stop_words\n",
    "\n",
    "stop_words_custom = list(set(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from utils.data import isolate_issueContext, clean_sentence, clean_issues, flatten_list\n",
    "\n",
    "issues_df['context_issue'] = issues_df['triplets'].apply(isolate_issueContext)\n",
    "issues_df['context_issue_cleaned'] = issues_df['context_issue'].apply(clean_issues)\n",
    "\n",
    "issues_stacked = issues_df.context_issue_cleaned.apply(pd.Series).stack().reset_index(level=1, drop=True).to_frame('Issues').dropna()\n",
    "issues_stacked = issues_df.drop('context_issue_cleaned', axis=1).join(issues_stacked, how='inner').reset_index(drop=True) ## To avoid missing Issues use inner join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model = SentenceTransformer('bert-base-nli-stsb-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_embeddings = emb_model.encode(flatten_list(issues_df['context_issue_cleaned'].values), show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-MAP/ HDBscan\n",
    "- Found to be much more accurate than original KMeans algorithms\n",
    "- be right for what it can, and defer on anything that it couldnâ€™t have sufficient confidence in\n",
    "- classifies many points/docs as \"noise\"\n",
    "- [UMAP documentation link](https://umap-learn.readthedocs.io/en/latest/clustering.html)\n",
    "- [HDBScan documentation link](https://hdbscan.readthedocs.io/en/latest/parameter_selection.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import hdbscan\n",
    "import umap\n",
    "\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.1, min_df=0.0015, ngram_range=(1,2)) #\n",
    "feature_matrix = tfidf_vectorizer.fit_transform(flatten_list(issues_df['context_issue_cleaned'].values))\n",
    "print(feature_matrix.shape)\n",
    "\n",
    "umap_model = umap.UMAP(n_neighbors=30,\n",
    "                            min_dist=0.0,\n",
    "                            n_components=10,\n",
    "                            metric='hellinger')\n",
    "umap_embeddings = umap_model.fit_transform(feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = hdbscan.HDBSCAN(min_cluster_size=15, \n",
    "                          min_samples=1,\n",
    "                          cluster_selection_method='eom').fit(umap_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df = pd.DataFrame(flatten_list(issues_df['context_issue_cleaned'].values), columns=[\"Doc\"])\n",
    "docs_df['Topic'] = cluster.labels_\n",
    "docs_df['Doc_ID'] = range(len(docs_df))\n",
    "docs_per_topic = docs_df.groupby(['Topic'], as_index = False).agg({'Doc': ' '.join})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data import c_tf_idf\n",
    "\n",
    "tf_idf, count = c_tf_idf(docs_per_topic.Doc.values, m=len(flatten_list(issues_df['context_issue_cleaned'].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data import extract_top_n_words_per_topic, extract_topic_sizes\n",
    "\n",
    "top_n_words = extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20)\n",
    "topic_sizes = extract_topic_sizes(docs_df); topic_sizes.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "data = flatten_list(issues_df['context_issue_cleaned'].values)\n",
    "                    \n",
    "for i in range(10):\n",
    "    # Calculate cosine similarity\n",
    "    similarities = cosine_similarity(tf_idf.T)\n",
    "    np.fill_diagonal(similarities, 0)\n",
    "\n",
    "    # Extract label to merge into and from where\n",
    "    topic_sizes = docs_df.groupby(['Topic']).count().sort_values(\"Doc\", ascending=False).reset_index()\n",
    "    topic_to_merge = topic_sizes.iloc[-1].Topic\n",
    "    topic_to_merge_into = np.argmax(similarities[topic_to_merge + 1]) - 1\n",
    "\n",
    "    # Adjust topics\n",
    "    docs_df.loc[docs_df.Topic == topic_to_merge, \"Topic\"] = topic_to_merge_into\n",
    "    old_topics = docs_df.sort_values(\"Topic\").Topic.unique()\n",
    "    map_topics = {old_topic: index - 1 for index, old_topic in enumerate(old_topics)}\n",
    "    docs_df.Topic = docs_df.Topic.map(map_topics)\n",
    "    docs_per_topic = docs_df.groupby(['Topic'], as_index = False).agg({'Doc': ', '.join})\n",
    "\n",
    "    # Calculate new topic words\n",
    "    m = len(data)\n",
    "    tf_idf, count = c_tf_idf(docs_per_topic.Doc.values, m)\n",
    "    top_n_words = extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20)\n",
    "\n",
    "print(topic_sizes.count())\n",
    "topic_sizes = extract_topic_sizes(docs_df); topic_sizes.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data import clean_dups, word_count, clear_redundent_unigrams\n",
    "\n",
    "top_n_words = extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=6)\n",
    "\n",
    "for cluster, topics in top_n_words.items():\n",
    "    # clear duplicates\n",
    "    topics = [clean_dups(top) for top, score in topics]\n",
    "    \n",
    "    # remove duplicates, list(set) loses ordering\n",
    "    final_topics = []\n",
    "    [final_topics.append(x) for x in topics if x not in final_topics]\n",
    "    \n",
    "    # clear unigrams that repeat in bigrams\n",
    "    final_topics = clear_redundent_unigrams(final_topics)\n",
    "    \n",
    "    # return top 3 words as string\n",
    "    topic_dict[cluster] = ', '.join(final_topics[0:3])\n",
    "    \n",
    "topics = pd.DataFrame(topic_dict, index=[0]).T.reset_index()\n",
    "topics.iloc[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = pd.merge(docs_df, topics, left_on='Topic', right_on='index').drop(['Doc_ID', 'index'], axis=1)\n",
    "cluster_df.columns = ['Issue', 'Cluster', 'Topic']\n",
    "print('Total Clusters: ', cluster_df['Topic'].value_counts().count())\n",
    "cluster_df['Topic'].value_counts().head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge \"Noise\" cluster with most similar clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_inds = cluster_df[cluster_df['Cluster'] == -1].index\n",
    "\n",
    "noise = cluster_df.loc[noise_inds].drop(['Cluster', 'Topic'], axis=1)\n",
    "classified = cluster_df.drop(noise_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute feature matrix for previous \"noise\"\n",
    "feature_matrix = tfidf_vectorizer.transform(noise['Issue'].values)\n",
    "\n",
    "# compute topic feature matrix for previous topics (ignoring \"noise\" i.e. -1)\n",
    "topic_matrix = tfidf_vectorizer.transform(docs_per_topic.Doc.values[1:])\n",
    "\n",
    "similarity = cosine_similarity(feature_matrix, topic_matrix)\n",
    "top = np.argmax(similarity, axis=1)\n",
    "\n",
    "# get top cluster and scores for all docs\n",
    "top_scores = [(x, similarity[i][x]) for i, x in enumerate(top)]\n",
    "\n",
    "# if tf_idf score over 5% add to new cluster else keep as noise\n",
    "noise['Cluster'] = [x[0] if x[1] > 0.05 else -1 for x in top_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = pd.merge(noise, topics, left_on='Cluster', right_on='index')\n",
    "noise = noise.drop('index', axis=1)\n",
    "noise.columns = ['Issue', 'Cluster', 'Topic']\n",
    "\n",
    "noise.loc[noise['Cluster'] == -1, 'Topic'] = 'Noise'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = classified.append(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_gen = (i for i in cluster_df['Topic'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = next(topic_gen)\n",
    "print(topic)\n",
    "print('Instances: ', cluster_df[cluster_df['Topic'] == topic]['Topic'].count())\n",
    "\n",
    "cluster_df[cluster_df['Topic'] == topic]['Issue'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_df = pd.merge(issues_stacked[['Case Number', 'Date', 'Severity', 'Customer Request', 'Issues']],\n",
    "                    cluster_df[['Issue', 'Cluster', 'Topic']], \n",
    "                    left_on='Issues',\n",
    "                    right_on='Issue',\n",
    "                    how='inner').drop_duplicates().reset_index()\n",
    "\n",
    "bi_cluster = cases_df.groupby('Case Number')['Cluster'].apply(list)\n",
    "tfidf_cluster = cases_df.groupby('Case Number')['Topic'].apply(set)\n",
    "cases_df = pd.merge(issues_df, bi_cluster, on='Case Number', how='left').merge(tfidf_cluster, on='Case Number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('spacy_ner_may11/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = cases_df.sample(1)\n",
    "spacy.displacy.render(nlp(sample['Customer Request'].values[0]), style='ent')\n",
    "\n",
    "for trip in ast.literal_eval(sample['triplets'].values[0]):\n",
    "    print('\\n', '-'*100)\n",
    "    print(f'PRODUCT: {trip[0]}')\n",
    "    print(f'RELATION: {trip[1]}')\n",
    "    print(f'TEXT: {trip[2]}')\n",
    "\n",
    "if sample['Cluster'].values[0] == [-1]:\n",
    "    print('Cluster Unassigned')\n",
    "else:\n",
    "    print('\\nUMap Clusters: ', sample['Topic'].values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans Clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_wcss(data, rand): \n",
    "    wcss = []\n",
    "    for n in tqdm(range(2, 100)):\n",
    "        kmeans = KMeans(n_clusters=n, random_state=rand)\n",
    "        kmeans.fit(X=data)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "    \n",
    "    plt.figure()\n",
    "    sns.lineplot(range(2, 100), wcss)\n",
    "    plt.title('Within Cluster Sum of Squared Error');\n",
    "    plt.xlabel(\"Number of cluster\")\n",
    "    plt.ylabel(\"SSE\")\n",
    "    plt.show()\n",
    "    return wcss\n",
    "\n",
    "def optimal_number_of_clusters(wcss):\n",
    "    x1, y1 = 2, wcss[0]\n",
    "    x2, y2 = 100, wcss[len(wcss)-1]\n",
    "    distances = []\n",
    "    \n",
    "    for i in tqdm(range(len(wcss))):\n",
    "        x0 = i+2\n",
    "        y0 = wcss[i]\n",
    "        numerator = abs((y2-y1)*x0 - (x2-x1)*y0 + x2*y1 - y2*x1)\n",
    "        denominator = math.sqrt((y2 - y1)**2 + (x2 - x1)**2)\n",
    "        distances.append(numerator/denominator)\n",
    "        \n",
    "    return distances.index(max(distances)) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"K value not provided, estimating optimal no of clusters\")\n",
    "print(\"calculating the within clusters sum-of-squares\")\n",
    "sum_of_squares = calculate_wcss(issue_embeddings, 43) #, op_dir, filename\n",
    "print(\"calculating the optimal number of clusters\")\n",
    "n = optimal_number_of_clusters(sum_of_squares)\n",
    "print(\"Optimal no of clusters : \", n)\n",
    "\n",
    "kmeans = KMeans(n_clusters = n, random_state=43)\n",
    "y_kmeans = kmeans.fit_predict(issue_embeddings)\n",
    "y=y_kmeans+1\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_clusters = int(n*1.5)\n",
    "\n",
    "#clustering_model = KMeans(n_clusters=n, random_state=26)\n",
    "clustering_model = KMeans(n_clusters=extended_clusters, random_state=0)\n",
    "\n",
    "clustering_model.fit(issue_embeddings)\n",
    "cluster_assignment = clustering_model.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterdf = pd.DataFrame()\n",
    "clusterdf['Issues']=flatten_list(issues_df['context_issue_cleaned'].values)\n",
    "#clusterdf['Issues']=flatten_list(issues_df['text_clean'].values)\n",
    "\n",
    "clusterdf['cluster']=cluster_assignment\n",
    "print(clusterdf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterdf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterdf['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Tf-idf matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix = tfidf_vectorizer.fit_transform(clusterdf['Issues'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"K value not provided, estimating optimal no of clusters\")\n",
    "print(\"calculating the within clusters sum-of-squares\")\n",
    "sum_of_squares = calculate_wcss(feature_matrix, 43) #, op_dir, filename\n",
    "print(\"calculating the optimal number of clusters\")\n",
    "n = optimal_number_of_clusters(sum_of_squares)\n",
    "print(\"Optimal no of clusters : \", n)\n",
    "\n",
    "kmeans = KMeans(n_clusters = n, random_state=43)\n",
    "y_kmeans = kmeans.fit_predict(feature_matrix)\n",
    "y=y_kmeans+1\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_model = KMeans(n_clusters=n, random_state=0)\n",
    "\n",
    "clustering_model.fit(feature_matrix)\n",
    "cluster_assignment = clustering_model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterdf = pd.DataFrame()\n",
    "clusterdf['Issues']=flatten_list(issues_df['context_issue_cleaned'].values)\n",
    "#clusterdf['Issues']=flatten_list(issues_df['text_clean'].values)\n",
    "\n",
    "clusterdf['cluster']=cluster_assignment\n",
    "print(clusterdf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "cluster_centers = clustering_model.cluster_centers_\n",
    "\n",
    "# Display the top_n terms in that cluster\n",
    "key_terms = []\n",
    "for i in range(n):\n",
    "    # Sort the terms and print top_n terms\n",
    "    center_terms = dict(zip(terms, list(cluster_centers[i])))\n",
    "    sorted_terms = sorted(center_terms, key=center_terms.get, reverse=True)\n",
    "    key_terms.append(sorted_terms[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_terms = pd.DataFrame([', '.join(words) for words in key_terms], columns=['Top Terms']).reset_index()\n",
    "tfidf_clusters = pd.merge(clusterdf, cluster_terms, left_on='cluster', right_on='index')\n",
    "tfidf_clusters.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bi-gram/Uni-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data import get_wordnet_pos\n",
    "\n",
    "unq_clust = clusterdf[\"cluster\"].unique()\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "label_df = pd.DataFrame()\n",
    "\n",
    "for uc in tqdm(unq_clust):\n",
    "    try:\n",
    "        tmp_clust_df = clusterdf.loc[clusterdf['cluster']==uc].copy()\n",
    "        temp_clust_corpus = list(set(tmp_clust_df['Issues'].dropna().to_list()))\n",
    "        \n",
    "        chunks_list =[]\n",
    "        wc_dict = Counter()\n",
    "        c_vec = CountVectorizer(ngram_range=(2, 2)) #min_n=1, max_n=2 #ngram_range=(1, 5) , min_df=0.5\n",
    "        for chunks in temp_clust_corpus:\n",
    "            words = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in word_tokenize(chunks)]\n",
    "            wc_dict.update(word for word in words)\n",
    "            chunks_list.append(words)\n",
    "        \n",
    "        label = wc_dict.most_common(1)[0][0]\n",
    "        \n",
    "        bigrams = c_vec.fit_transform(temp_clust_corpus)\n",
    "        vocab = c_vec.vocabulary_\n",
    "        count_values = bigrams.toarray().sum(axis=0)\n",
    "        bigram_count = sorted([(count_values[i],k) for k,i in vocab.items()], reverse=True)\n",
    "        \n",
    "        bigram_label = bigram_count[0][1]\n",
    "        \n",
    "        flag_var = 2 if label in bigram_label.split(\" \") else 1 ## or (bigram_label.find(label)>=0)\n",
    "        #& label != 'not'\n",
    "        \n",
    "        lb_df = pd.DataFrame({'Cluster2': bigram_label, 'Cluster1': label, 'Cluster_Flag': flag_var, 'cluster' : uc}, index=[0]) #, 'Cluster1_root': root\n",
    "        label_df = label_df.append(lb_df, ignore_index = True)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df.loc[label_df['Cluster_Flag']==1, 'Cluster'] = label_df['Cluster1']\n",
    "label_df.loc[label_df['Cluster_Flag']==2, 'Cluster'] = label_df['Cluster2']\n",
    "label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_clusters = pd.merge(clusterdf, label_df, on='cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.1, min_df=0.001, ngram_range=(1,2))\n",
    "tfidf_vectorizer.fit(issues_df['Customer Request'].values)\n",
    "\n",
    "\n",
    "unq_clust = clusterdf[\"cluster\"].unique()\n",
    "unq_clusterdf = pd.DataFrame()\n",
    "\n",
    "for uc in tqdm(unq_clust):\n",
    "    tmp_clust_df = clusterdf.loc[clusterdf['cluster']==uc].copy()\n",
    "    chunks = [word_tokenize(x) for x in list(set(tmp_clust_df['Issues'].dropna().to_list()))]\n",
    "    chunk_corpus = ([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in flatten_list(chunks)])\n",
    "    \n",
    "    tmp_df = pd.DataFrame({\"cluster\": uc, 'text': [', '.join(chunk_corpus)]})\n",
    "    unq_clusterdf = unq_clusterdf.append(tmp_df, ignore_index = True) \n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.transform(unq_clusterdf.text.dropna())\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix_lst = list(tfidf_matrix)\n",
    "\n",
    "tfidf_terms = tfidf_vectorizer.get_feature_names()\n",
    "term_lst = []\n",
    "score_lst = []\n",
    "\n",
    "for e in tfidf_matrix_lst:\n",
    "    top_n_term_indx = np.argsort(e.toarray())[0][::-1][:4]\n",
    "    term_lst.append(list(map(tfidf_terms.__getitem__, top_n_term_indx)))\n",
    "    score_lst.append(list(map(e.toarray()[0].__getitem__, top_n_term_indx)))\n",
    "\n",
    "unq_clust = pd.DataFrame(unq_clust)\n",
    "unq_clust['Cluster_tfidf'] = term_lst\n",
    "unq_clust['Score_tfidf'] = score_lst\n",
    "\n",
    "labeled_clusters = pd.merge(labeled_clusters, unq_clust, left_on='cluster', right_on=0)\n",
    "labeled_clusters['Cluster_tfidf'] = list(labeled_clusters['Cluster_tfidf'].apply(lambda x: ', '.join(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_df = pd.merge(issues_stacked[['Case Number', 'Date', 'Severity', 'Customer Request', 'Issues']],\n",
    "                    labeled_clusters[['Issues', 'cluster', 'Cluster', 'Cluster_tfidf']], \n",
    "                    on='Issues', \n",
    "                    how='inner').drop_duplicates().reset_index()\n",
    "\n",
    "bi_cluster = cases_df.groupby('Case Number')['Cluster'].apply(list)\n",
    "tfidf_cluster = cases_df.groupby('Case Number')['Cluster_tfidf'].apply(set)\n",
    "cases_df = pd.merge(issues_df, bi_cluster, on='Case Number', how='left').merge(tfidf_cluster, on='Case Number')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
