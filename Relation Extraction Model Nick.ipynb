{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9d8e210",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import defaultdict\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import spacy\n",
    "#import neuralcoref\n",
    "assert spacy.__version__ == '2.1.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871a9219",
   "metadata": {},
   "source": [
    "##### To download correct Spacy & Neural Coref Version\n",
    "```\n",
    "pip uninstall spacy \n",
    "pip uninstall neuralcoref\n",
    "pip install spacy==2.1.0 \n",
    "pip install neuralcoref --no-binary neuralcoref\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d7506c",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfd43c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rawdata1 = pd.read_csv('2018-JAN-June.csv', encoding = \"utf-8\", engine=\"python\")\n",
    "new_rawdata2 = pd.read_csv('2018-JULY-DEC.csv', encoding = \"utf-8\", engine=\"python\")\n",
    "new_rawdata3 = pd.read_csv('2019.csv', encoding = \"utf-8\", engine=\"python\")\n",
    "new_rawdata4 = pd.read_csv('2020.csv', encoding = \"utf-8\", engine=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "137293d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rawData_all = new_rawdata1.append([new_rawdata2, new_rawdata3, new_rawdata4])\n",
    "#new_rawData_all = new_rawdata3\n",
    "new_rawdata = new_rawData_all.loc[((new_rawData_all['Country']==\"USA\")|\n",
    "                                (new_rawData_all['Country']==\"United Kingdom\")|\n",
    "                                (new_rawData_all['Country']==\"Canada\")|\n",
    "                                (new_rawData_all['Country']==\"South Africa\")|\n",
    "                                (new_rawData_all['Country']==\"New Zealand\")|\n",
    "                                (new_rawData_all['Country']==\"Ireland\")|\n",
    "                                (new_rawData_all['Country']==\"Australia\"))&\\\n",
    "                               (new_rawData_all['Category']==\"10 - Troubleshooting*\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b520bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2914, 9)\n",
      "['PTCCB - MCCBS,EARTH LEAK&SWITC']\n"
     ]
    }
   ],
   "source": [
    "rawdf = new_rawdata.loc[new_rawdata['Product Line'].isnull()==False][['Case Number','Severity','Priority','Country','Date/Time Opened','Product Line','Subject','Customer Request','Answer To Customer']].reset_index(drop=True)\n",
    "print(rawdf.shape)\n",
    "print(rawdf['Product Line'].unique())\n",
    "rawdf.drop_duplicates(inplace=True)\n",
    "rawdf.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc7b2257",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert the Severity and Proirity values from Categorical to Ordinal \n",
    "rawdf.loc[rawdf['Priority'].isnull()==True,\"PRIORITY\"]=0\n",
    "rawdf.loc[rawdf['Priority']=='Normal',\"PRIORITY\"]=1\n",
    "rawdf.loc[rawdf['Priority']=='Serious',\"PRIORITY\"]=2\n",
    "rawdf.loc[rawdf['Priority']=='High',\"PRIORITY\"]=3\n",
    "\n",
    "rawdf.loc[rawdf['Severity'].isnull()==True,\"SEVERITY\"]=0\n",
    "rawdf.loc[rawdf['Severity']=='Normal',\"SEVERITY\"]=1\n",
    "rawdf.loc[rawdf['Severity']=='High Financial Impact',\"SEVERITY\"]=5\n",
    "rawdf.loc[rawdf['Severity']=='Shutdown / Downtime in Operations',\"SEVERITY\"]=6\n",
    "rawdf.loc[rawdf['Severity']=='Customer Relationship Risk',\"SEVERITY\"]=9\n",
    "rawdf.loc[rawdf['Severity']=='Potential Safety Issue',\"SEVERITY\"]=10\n",
    "rawdf.loc[rawdf['Severity']=='Potential cyber issue',\"SEVERITY\"]=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a792b628",
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdf['Date raw'] = pd.to_datetime(rawdf['Date/Time Opened'], format='%d/%m/%Y %I:%M %p')\n",
    "rawdf['Date'] = pd.to_datetime(rawdf['Date raw'].dt.strftime('1/%m/%Y'), format='%d/%m/%Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e16813",
   "metadata": {},
   "source": [
    "# NER Training \n",
    "- NLP module train to recognize all SE reference numbers in a customer request\n",
    "    - Begin with pre-trained Spacy Language Model (medium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "f68c4a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: spacy.tokens.span.Span size changed, may indicate binary incompatibility. Expected 72 from C header, got 80 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "100%|██████████| 40155833/40155833 [00:03<00:00, 13079957.33B/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x7f7390ef9668>"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import en_core_web_md\n",
    "\n",
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1042,
   "id": "32910ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('./spacy_ner_may11')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4f7df1",
   "metadata": {},
   "source": [
    "### Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1053,
   "id": "094f0155",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.util import minibatch, compounding\n",
    "from spacy.util import decaying\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def custom_optimizer(optimizer, learn_rate=0.0001, beta1=0.9, beta2=0.999, eps=1e-8, L2=1e-6, max_grad_norm=1.0):\n",
    "    \"\"\"\n",
    "    Function to customizer spaCy default optimizer\n",
    "    \"\"\"\n",
    "    \n",
    "    optimizer.learn_rate = learn_rate\n",
    "    optimizer.beta1 = beta1\n",
    "    optimizer.beta2 = beta2\n",
    "    optimizer.eps = eps\n",
    "    optimizer.L2 = L2\n",
    "    optimizer.max_grad_norm = max_grad_norm\n",
    "    \n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def train_ner(TRAINING_DATA, EPOCHS, lr, log_every=100):\n",
    "    optimizer = nlp.entity.create_optimizer()\n",
    "    \n",
    "    unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    \n",
    "    # TRAINING THE MODEL\n",
    "    with nlp.disable_pipes(*unaffected_pipes):\n",
    "        \n",
    "        optimizer = nlp.resume_training(component_cfg={\"ner\": {\"conv_window\": 3, \"self_attn_depth\": 2}})\n",
    "        optimizer = custom_optimizer(optimizer, lr)\n",
    "        \n",
    "        dropout = decaying(0.6, 0.2, 1e-4)\n",
    "        \n",
    "        for i in range(EPOCHS):\n",
    "            # shuufling examples  before every iteration\n",
    "            random.shuffle(TRAINING_DATA)\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(TRAINING_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                            texts,  \n",
    "                            annotations,  \n",
    "                            drop=next(dropout),\n",
    "                            sgd=optimizer,\n",
    "                            losses=losses,\n",
    "                        )\n",
    "            if (i+1) % log_every == 0: print(f\"Iteration {i+1} ==> Loss: \", losses['ner'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b031f0b3",
   "metadata": {},
   "source": [
    "### Unsupervised Dataset Gen\n",
    "- Takes patterns and builds dataset from Customer Request\n",
    "    - Enter keywords e.g. breaker, generator\n",
    "    - Regex search for catalog #'s e.g. HOM120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1044,
   "id": "949e6f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "def build_ner_training(txt, pattern, coref=False):\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        # set singular and plural patterns as well as base part catcher\n",
    "        \n",
    "        pattern1 = [{'DEP':'ROOT','OP':'?'},\n",
    "                {'DEP':'meta','OP':'?'},\n",
    "                {'DEP':'compound','OP':'?'},\n",
    "                {'DEP':'nsubj','OP':'?'},\n",
    "                {'LOWER': pattern}]\n",
    "    \n",
    "        pattern2 = [{'DEP':'ROOT','OP':'?'},\n",
    "                {'DEP':'meta','OP':'?'},\n",
    "                {'DEP':'compound','OP':'?'},\n",
    "                {'DEP':'nsubj','OP':'?'},\n",
    "                {'LOWER': pattern + 's'}]\n",
    "        \n",
    "        # general regex to capture prt nbr\n",
    "        pattern3 = [{'DEP':'ROOT','OP':'?'},\n",
    "                {'DEP':'meta','OP':'?'},\n",
    "                {'DEP':'compound','OP':'?'},\n",
    "                {'DEP':'nsubj','OP':'?'},\n",
    "                {'TEXT': {'REGEX': '(\\w*\\d[\\w\\d]+)'}}]\n",
    "        \n",
    "    \n",
    "        if len(pattern.split(' ')) == 2:\n",
    "            # drop last element\n",
    "            pattern1.pop()\n",
    "            pattern2.pop()\n",
    "            \n",
    "            # add new multi word pattern\n",
    "            pattern1.append({'LOWER': pattern.split(' ')[0]})\n",
    "            pattern1.append({'LOWER': pattern.split(' ')[1]})\n",
    "            pattern2.append({'LOWER': pattern.split(' ')[0]})\n",
    "            pattern2.append({'LOWER': pattern.split(' ')[1] + 's'})\n",
    "    \n",
    "        matcher.add(\"PRODUCT\", None, pattern1, pattern2, pattern3)\n",
    "    \n",
    "    TRAINING_DATA = []\n",
    "\n",
    "    for i, doc in enumerate(nlp.pipe(txt)):\n",
    "        \n",
    "        # Resolve coreferences\n",
    "        if coref:\n",
    "            doc = nlp(doc._.coref_resolved)\n",
    "        \n",
    "        # Match on the doc and create a list of matched spans\n",
    "        spans = [doc[start:end] for match_id, start, end in matcher(doc)]\n",
    "        \n",
    "        # Get (start character, end character, label) tuples of matches\n",
    "        entities = [(span.start_char, span.end_char, \"PRODUCT\") for span in spans]\n",
    "        \n",
    "        # Format the matches as a (doc.text, entities) tuple\n",
    "        training_example = (doc.text, {\"entities\": entities})\n",
    "        \n",
    "        # Append the example to the training data\n",
    "        TRAINING_DATA.append(training_example)\n",
    "        \n",
    "    print(f'{len(TRAINING_DATA)} Examples Captured')\n",
    "        \n",
    "    return TRAINING_DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc57042",
   "metadata": {},
   "source": [
    "#### Generate unsupervised Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1055,
   "id": "a5a4cc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1248 Examples Captured\n"
     ]
    }
   ],
   "source": [
    "patterns = ['breaker', 'generator', 'trip unit']\n",
    "\n",
    "flatten_list = lambda docs: [x for sublist in docs for x in sublist]\n",
    "\n",
    "train_docs = []\n",
    "for pat in patterns:\n",
    "    #docs = [x for x in flatten_list(rawdf['CustReqCorpus']) if pat in x]\n",
    "    docs = [clean_request(x) for x in rawdf['Customer Request'] if pat in x]\n",
    "    train_docs.append(docs)\n",
    "    \n",
    "train_docs = flatten_list(train_docs)\n",
    "    \n",
    "TRAIN_DATA = build_ner_training(train_docs, patterns, coref=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1056,
   "id": "92f6bf3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Your request has been received by your CIC Team and is currently under review Return Authorization Request Detailed Information Date PM EST March Requestor soltero Heroldo heroldo soltero vertivco com Account LIEBERT NORTH AMERICA INC Account Number PO Number LBR747876 Return Reason For return of Inoperative material Credit will be determined after receipt and inspection of material If unit is found to be out of warranty unit may be returned to you at your expense If found to be operative after testing charges may be invoiced to the Distributor Under the heading Return Product if Credit Denied if you choose YES the product will be returned to you at your expense If you choose NO the product will be scrapped by Schneider Electric Please provide detailed description of Observed Failure Application for each product listed Return Reason details Circuit breaker does not make the change of state RG No Line Catalog Product Return Product if Credit Denied Return Qty Original Order Requested Unit Price Validation error 548755P31 Yes lbr747876 Line Catalog Product Return Product if Credit Denied Return Qty Original Order Requested Unit Price Validation error 548755P31 Yes lbr747876 ',\n",
       " {'entities': [(253, 269, 'PRODUCT'),\n",
       "   (260, 269, 'PRODUCT'),\n",
       "   (838, 868, 'PRODUCT'),\n",
       "   (845, 868, 'PRODUCT'),\n",
       "   (853, 868, 'PRODUCT'),\n",
       "   (861, 868, 'PRODUCT'),\n",
       "   (1026, 1035, 'PRODUCT'),\n",
       "   (1040, 1049, 'PRODUCT'),\n",
       "   (1167, 1176, 'PRODUCT'),\n",
       "   (1181, 1190, 'PRODUCT')]})"
      ]
     },
     "execution_count": 1056,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DATA[40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1058,
   "id": "54866c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25 ==> Loss:  16136.99470347059\n",
      "Iteration 50 ==> Loss:  10653.724902643924\n",
      "Iteration 75 ==> Loss:  9216.664163781461\n",
      "Iteration 100 ==> Loss:  9179.992964203926\n",
      "Iteration 125 ==> Loss:  9647.478213392704\n",
      "Iteration 150 ==> Loss:  9937.395929641778\n",
      "Iteration 175 ==> Loss:  10657.21062855414\n",
      "Iteration 200 ==> Loss:  10773.646825882599\n",
      "Iteration 225 ==> Loss:  8691.716869966018\n",
      "Iteration 250 ==> Loss:  8912.381784371002\n"
     ]
    }
   ],
   "source": [
    "train_ner(TRAIN_DATA, 250, lr=0.00001, log_every=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b66a3f8",
   "metadata": {},
   "source": [
    "## Train on annotated data\n",
    "- Small sample dataset generated to fine-tune model further.\n",
    "    - generate using ```spacy_annotator``` lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "dcaa29ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "TRAIN_DATA = pd.read_csv('annot_2019v1.csv')['annotations']\n",
    "TRAIN_DATA = list(TRAIN_DATA.apply(lambda x: ast.literal_eval(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9daf8ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('keeps getting a fail on ground fault through second injection wants to set it up according to the coordination studies as far as he knows the breaker is set up according to the studies is there a chance that the setting may be conflicting with the tests Ig J 4 OFF Ir 1 8 Isd 6 1 OFF Ii 8',\n",
       " {'entities': [(138, 149, 'PRODUCT')]})"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DATA[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b33182c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f5b463",
   "metadata": {},
   "source": [
    "#### Add Emal Garbage label to NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1062,
   "id": "6c11c0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "add_ents = ['PRODUCT','EMAIL GARBAGE']\n",
    "\n",
    "for ent in add_ents:\n",
    "    if 'extra_labels' in ner.cfg and ent in ner.cfg['extra_labels']:\n",
    "        pass\n",
    "    else:\n",
    "        ner.add_label(ent)\n",
    "    \n",
    "ner.cfg['extra_labels'] = add_ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "bef3af7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = (i for i in TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9b08133c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE:\n",
      " We have a CM3000 with a ecc21 but cannot change the IP from the display itself what would cause this is it a sign of a bad ethernet card  \n",
      " \n",
      "LABELS:\n",
      "CM3000 with a ecc21\n"
     ]
    }
   ],
   "source": [
    "instance = next(train_generator)\n",
    "txt, ents = instance\n",
    "inds = [x[:2] for x in list(ents.values())[0]]\n",
    "\n",
    "print('EXAMPLE:\\n', txt, '\\n', '\\nLABELS:')\n",
    "for ex in inds:\n",
    "    print(txt[ex[0]:ex[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96d85a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100 ==> Loss:  785.9439191968231\n",
      "Iteration 200 ==> Loss:  246.44435745156196\n",
      "Iteration 300 ==> Loss:  61.73124403851792\n",
      "Iteration 400 ==> Loss:  22.847422277885567\n",
      "Iteration 500 ==> Loss:  10.302024816423266\n",
      "Iteration 600 ==> Loss:  15.612178293243394\n",
      "Iteration 700 ==> Loss:  20.15935285399992\n"
     ]
    }
   ],
   "source": [
    "nlp.begin_training()\n",
    "train_ner(TRAIN_DATA, 1000, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b836c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk('./spacy_ner_may17')\n",
    "print('Saved NER Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef40f44",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422967f0",
   "metadata": {},
   "source": [
    "#### Load Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60717d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('./spacy_ner_may11')\n",
    "#neuralcoref.add_to_pipe(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8dfe3166",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_txts = (item for item in rawdf['Customer Request'].sample(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17a7dcaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Dear Team, </br>   </br> Would you please forward the feedback below to the applicable product support group for Powerpact molded case circuit breakers and request an explanation &amp; suggested response -   </br>   </br> \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Thank You in Advance,  \n",
       " KW  \n",
       "  \n",
       " \n",
       "   \n",
       "\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">EMAIL GARBAGE</span>\n",
       "</mark>\n",
       "Sent from my iPhone </br>   </br> Kurt Weiss  </br> Sales Executive  </br> Square D / Schneider Electric  </br> Roanoke, VA  </br>   </br> </br> </br>Begin forwarded message:</br>  </br> </br> From: &quot;Daniel Gillispie&quot; &lt; Daniel_Gillispie@cargill.com &gt;</br> To: &quot;Kurt Weiss&quot; &lt; kurt.weiss@se.com &gt;</br> Subject: Question for the Square D company </br>  </br> </br>   </br> </br>   </br> </br> </br> [External email: Use caution with links and attachments] </br> </br>   </br> </br> </br> Kurt, </br>   </br> I had a question and a concern to forward on to the Square D company. </br>   </br> It took he most experienced electrical contractors in the Shenandoah Valley (Trumbo Electric) approx. four hours to terminate some existing 500MCM cabling to (2) 800 amp \n",
       "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    MJA36800 circuit breakers\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
       "</mark>\n",
       " this weekend.  This was entirely due to the new lugs provided with the circuit breaker.  It is becoming a great concern to me that Square D, in the pursuit of design change, has begun selling termination lugs with such tight tolerances that it will not fit the cabling specified by the NEC.  Ease of installation and maintenance should be a primary factor in design.  I have these lugs here if you or anybody from Square D would like to look at them. </br>   </br> The new lugs I am referring too are stamped #3/0 to 500MCM.  These lugs were provided with the new \n",
       "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    MJA36800 circuit breakers\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
       "</mark>\n",
       ".  The holes in the new lugs have an oblong shape, and do not readily accept the installation of an existing, round, 500mcm conductor.  It is literally putting a round peg in an oblong hole.  It does not fit the cables that the lugs are stamped for.  If you look at picture 083823, I have laid the old lug on top of the new lug so you can see all the area that was given up in the new lug because of the two flat spots. After much struggle, the contractors were finally able to land 500MCM cables onto the back row of lugs.  If you will look closely at picture 083838,  you will notice that the back row of holes contains a flat spot inside the hole.  The diameter of the back holes,  measuring from the flat spot of the hole to the opposite side is .785”.  According to the NEC, the diameter of a 500MCM cable copper is .813”.  The .813” cable diameter listed by the NEC is larger than the  .785” diameter hole allowed in the lugs Square D provided with the 800 amp \n",
       "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    MJA36800 circuit breakers???????  \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
       "</mark>\n",
       "Why is this????       The contractors were absolutely not able to land a 500MCM cable in the front hole of the new lug.  If you will look closely at picture 083838,  you will notice that the front hole contains two flat spots inside the hole.  The diameter of the front holes,  measuring from the flat spot of the hole to the opposite flat spot of the hole .746” .  According to the NEC, the diameter of a 500MCM cable copper is .813”.  The .813” cable diameter listed by the NEC is much larger than the  .746” diameter hole allowed in the lugs Square D provided with the 800 amp \n",
       "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    MJA36800 circuit breakers??????\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
       "</mark>\n",
       "?  Why is this??? Trumbo Electric even cut off existing end of the 500mcm cable to have fresh copper to work with, and still could not fit the cable into front hole on the new lug. We ended up having to pull the old lugs out of the new \n",
       "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    circuit breaker\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
       "</mark>\n",
       " to be able to land the three cables per pole.  \n",
       "   \n",
       "   \n",
       "   \n",
       " Please contact me with any questions or concerns. \n",
       "   \n",
       " Dan Gillispie \n",
       " Maintenance-Reliability Supervisor \n",
       " Cargill-Dayton Facility \n",
       " (540)-879-2601 \n",
       " daniel_gillispie@cargill.com   \n",
       "   \n",
       "   \n",
       "          \n",
       "         \n",
       "   \n",
       "   \n",
       "   \n",
       "   \n",
       "   \n",
       "   \n",
       "   \n",
       "          \n",
       "   \n",
       " \n",
       "______________________________________________________________________\n",
       "This email has been scanned by the Symantec Email Security.cloud service.\n",
       "______________________________________________________________________ \n",
       " \n",
       "   \n",
       " \n",
       " \n",
       "   \n",
       " \n",
       " \n",
       "  </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(next(sample_txts))\n",
    "\n",
    "spacy.displacy.render(doc, style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429876df",
   "metadata": {},
   "source": [
    "## Issue Finder\n",
    "- Start from transformers bert-large fine tuned Q/A model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ec29106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.7.0'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7561003e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dc629924074443c8a788b1f3b691d17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/443 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31125d2adf884251990d6636538e3aaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da2728a445d242d69aea6df2911b3c00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9fd8b9633c24f658e246e61db9a93de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f83d3e9d30c492cbd7b4c29b26f398a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccec092e",
   "metadata": {},
   "source": [
    "## Customer Request Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b11ba5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_issue(txt, ent, question, verbose=True):\n",
    "    \n",
    "    query = question + ent + '?'\n",
    "    \n",
    "    inputs = tokenizer(query, txt, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "    \n",
    "    outputs = model(**inputs)\n",
    "    answer_start_scores = outputs.start_logits\n",
    "    answer_end_scores = outputs.end_logits\n",
    "    \n",
    "    answer_start = torch.argmax(\n",
    "        answer_start_scores\n",
    "    )  # Get the most likely beginning of answer with the argmax of the score\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n",
    "    \n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "    confidence_raw = answer_start_scores[0][answer_start] + answer_end_scores[0][answer_end]\n",
    "    confidence_score = 1/(1 + np.exp(-confidence_raw.detach().numpy()))\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Answer: {answer}\")\n",
    "    return str(answer), confidence_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80a6679f",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_list = lambda docs: [x for sublist in docs for x in sublist]\n",
    "id_ents = lambda txt: [ent.text for ent in nlp(txt).ents if (ent.label_ == 'PRODUCT')]\n",
    "\n",
    "def clean_request(txt):\n",
    "    txt  = re.sub('[^A-Za-z0-9]+', ' ', txt)\n",
    "    txt = re.sub(r'\\b[0-9]+\\b\\s*', '', txt)\n",
    "    return txt\n",
    "\n",
    "def sentence_splitter(txt):\n",
    "    sents = sent_tokenize(txt)\n",
    "    sents = flatten_list([sent.split('\\n') for sent in sents])\n",
    "    return sents\n",
    "\n",
    "def split_request(txt):\n",
    "    \"\"\"\n",
    "    split sentences into chunks < 512\n",
    "    characters to feed to qa relation\n",
    "    extraction model.\n",
    "    \"\"\"\n",
    "    sent_staging = []\n",
    "    paragraph = []\n",
    "    for sent in sentence_splitter(txt):\n",
    "        sent = clean_request(sent)\n",
    "        staged_len = sum([len(chunk) for chunk in sent_staging])\n",
    "        if len(sent) + staged_len < 512:\n",
    "            sent_staging.append(sent)\n",
    "        else:\n",
    "            paragraph.append('. '.join(sent_staging))\n",
    "            if len(sent) < 512:\n",
    "                sent_staging = [sent]\n",
    "            \n",
    "    paragraph.append('. '.join(sent_staging))\n",
    "    return paragraph\n",
    "\n",
    "def resolve_relations(txt, THRESHOLD=0.9):\n",
    "    \"\"\"\n",
    "    loop through all relation/question pairs\n",
    "    in the query dict to extract key relation\n",
    "    values from request.\n",
    "    Return all pairs w/ score greater than\n",
    "    THRESHOLD value.\n",
    "    \"\"\"\n",
    "    query_dict = { \n",
    "        'CONTEXT' : \"What is the context of the \",\n",
    "        'ISSUE'   : \"What is the issue with the \",\n",
    "        'LOCATION': \"What is the location of the \"\n",
    "    }\n",
    "    \n",
    "    triplets = []\n",
    "    ents = set(id_ents(txt))\n",
    "    \n",
    "    # need to move upstream as this gets 'PRODUCT' triples\n",
    "    #  for every passage in large texts... to much noise\n",
    "    #if len(ents) == 0: ents = ['PRODUCT']\n",
    "        \n",
    "    for ent in ents:\n",
    "        for relation, question in query_dict.items():\n",
    "            ans, score = find_issue(txt, ent, question, verbose=False)\n",
    "            if score > THRESHOLD:\n",
    "                triplets.append(((ent, relation, ans, score)))\n",
    "    return triplets\n",
    "\n",
    "def process_triplets(triplets):\n",
    "    \"\"\"\n",
    "    loop through triplets append non-empty results\n",
    "    choose highest confidence product, response \n",
    "    pair for all responses \n",
    "    \"\"\"\n",
    "    final_triplets = []\n",
    "    answers = defaultdict(list)\n",
    "    \n",
    "    for trip in triplets:\n",
    "        if trip[2] != '': answers[trip[2]].append(trip)\n",
    "            \n",
    "    # remove duplicate answers \n",
    "    for ans in answers.keys():\n",
    "        final_triplets.append(sorted(\n",
    "            answers[ans], key=lambda x: x[2],\n",
    "            reverse=True)[0:3][0]\n",
    "            )\n",
    "        \n",
    "\n",
    "    return final_triplets\n",
    "    #return [trip[:3] for trip in final_triplets]\n",
    "    \n",
    "def process_request(txt):\n",
    "    chunks = split_request(txt)\n",
    "    if len(chunks) == 1:\n",
    "        triplets = (resolve_relations(txt))\n",
    "    else:\n",
    "        triplets = flatten_list([resolve_relations(chunk) for chunk in chunks if chunk != ''])\n",
    "    triplets = process_triplets(triplets)\n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f94895d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Chris </br> I was on site at Whitechapel yesterday and was presented with the attached commissioning test sheets for the NS breakers fitted to the Motor Control Centres we supplied to the site. </br> These tests are reporting that at least 14 off \n",
       "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    NSX100M\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
       "</mark>\n",
       "-TM-D integral trip facility is not operating when pressed with voltage present </br> The comments on the sizing of the overloads are for us to look at regarding the Amtech report now versus the one used to build the \n",
       "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    switchgear\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
       "</mark>\n",
       ". </br>   </br> Have you had any other reports of faulty internal trip facility on these \n",
       "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    breakers\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
       "</mark>\n",
       " as it looks as if there may be a batch issue? \n",
       " If someone was available to attend the site next week this would be great for customer relations whilst we look into the issue \n",
       " Can you let me know please and Pushpraj will be in touch to request new units \n",
       "   \n",
       " Regards \n",
       " Billy \n",
       "   \n",
       "   \n",
       " William Naismith \n",
       " Director | Balfour Beatty | Specialist Services | Power Systems \n",
       " T: +44 (0)141 880 2402 | M: +44 (0)7973 783  531 | E: william.naismith@balfourbeatty.com \n",
       " 5 Church hill Place, Canary Wharf, London E14 5HU \n",
       " PA: Ann Irvine| T: +44 (0)141 880 2405 | E: ann.irvine@balfourbeatty.com \n",
       " www.balfourbeatty.com |     @balfourbeatty |   LinkedIn \n",
       " </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('switchgear', 'ISSUE', 'not operating when pressed with voltage present', 0.9808951575498895), ('switchgear', 'LOCATION', 'whitechapel', 0.9691624712420791), ('NSX100', 'CONTEXT', 'tm d integral trip facility is not operating when pressed with voltage present', 0.9925898560999983), ('breakers', 'CONTEXT', 'it looks as if there may be a batch issue', 0.9475549028566288), ('breakers', 'ISSUE', 'a batch issue', 0.998371765172819)]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(next(sample_txts))\n",
    "\n",
    "spacy.displacy.render(doc, style='ent')\n",
    "print(process_request(doc.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ff4b93",
   "metadata": {},
   "source": [
    "# Test on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0cabba2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rawdata1 = pd.read_csv('2018-JAN-June.csv', encoding = \"utf-8\", engine=\"python\")\n",
    "new_rawdata4 = pd.read_csv('2020.csv', encoding = \"utf-8\", engine=\"python\")\n",
    "new_rawdata3 = pd.read_csv('2019.csv', encoding = \"utf-8\", engine=\"python\")\n",
    "new_rawdata2 = pd.read_csv('2018-JULY-DEC.csv', encoding = \"utf-8\", engine=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b71748c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rawData_all = new_rawdata1.append([new_rawdata2, new_rawdata3, new_rawdata4])\n",
    "#new_rawData_all = new_rawdata4\n",
    "new_rawdata = new_rawData_all.loc[((new_rawData_all['Country']==\"USA\")|\n",
    "                                (new_rawData_all['Country']==\"United Kingdom\")|\n",
    "                                (new_rawData_all['Country']==\"Canada\")|\n",
    "                                (new_rawData_all['Country']==\"South Africa\")|\n",
    "                                (new_rawData_all['Country']==\"New Zealand\")|\n",
    "                                (new_rawData_all['Country']==\"Ireland\")|\n",
    "                                (new_rawData_all['Country']==\"Australia\"))&\\\n",
    "                               (new_rawData_all['Category']==\"10 - Troubleshooting*\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6921e90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2914, 9)\n",
      "['PTCCB - MCCBS,EARTH LEAK&SWITC']\n"
     ]
    }
   ],
   "source": [
    "rawdf = new_rawdata.loc[new_rawdata['Product Line'].isnull()==False][['Case Number','Severity','Priority','Country','Date/Time Opened','Product Line','Subject','Customer Request','Answer To Customer']].reset_index(drop=True)\n",
    "print(rawdf.shape)\n",
    "print(rawdf['Product Line'].unique())\n",
    "rawdf.drop_duplicates(inplace=True)\n",
    "rawdf.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5dffe25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2451/2451 [26:48<00:00,  1.52it/s]  \n"
     ]
    }
   ],
   "source": [
    "rawdf['triplets'] = rawdf['Customer Request'].progress_apply(process_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56e8d18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdf['Date raw'] = pd.to_datetime(rawdf['Date/Time Opened'], format='%d/%m/%Y %I:%M %p')\n",
    "rawdf['Date'] = pd.to_datetime(rawdf['Date raw'].dt.strftime('1/%m/%Y'), format='%d/%m/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4911649b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Case Number</th>\n",
       "      <th>Severity</th>\n",
       "      <th>Priority</th>\n",
       "      <th>Country</th>\n",
       "      <th>Date/Time Opened</th>\n",
       "      <th>Product Line</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Customer Request</th>\n",
       "      <th>Answer To Customer</th>\n",
       "      <th>triplets</th>\n",
       "      <th>Date raw</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>46181136</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "      <td>USA</td>\n",
       "      <td>10/2/2018 3:41 am</td>\n",
       "      <td>PTCCB - MCCBS,EARTH LEAK&amp;SWITC</td>\n",
       "      <td>PJA36040CU43CE1ACMOLV and EBX510</td>\n",
       "      <td>Name: David Rosell\\nPhone: 7862264495\\nConcern...</td>\n",
       "      <td>He had the trip unit door closed.  With it ope...</td>\n",
       "      <td>[(EBX510, ISSUE, has changed the trip unit, 0....</td>\n",
       "      <td>2018-02-10 03:41:00</td>\n",
       "      <td>2018-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>48164862</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "      <td>USA</td>\n",
       "      <td>24/4/2018 3:11 am</td>\n",
       "      <td>PTCCB - MCCBS,EARTH LEAK&amp;SWITC</td>\n",
       "      <td>RJF36160U44A having nuisance tripping</td>\n",
       "      <td>Name: Albert Sarkis\\nPhone: (209) 744-1513\\nCo...</td>\n",
       "      <td>This breaker appears to have a damaged di/dt b...</td>\n",
       "      <td>[(RJF36160U44A, CONTEXT, having nuisance tripp...</td>\n",
       "      <td>2018-04-24 03:11:00</td>\n",
       "      <td>2018-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>46224073</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "      <td>USA</td>\n",
       "      <td>12/2/2018 8:22 pm</td>\n",
       "      <td>PTCCB - MCCBS,EARTH LEAK&amp;SWITC</td>\n",
       "      <td>TEX - Square D warranty claim</td>\n",
       "      <td>Please process the attached supplier recovery ...</td>\n",
       "      <td>Initiate TEX</td>\n",
       "      <td>[]</td>\n",
       "      <td>2018-02-12 20:22:00</td>\n",
       "      <td>2018-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>46237499</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "      <td>USA</td>\n",
       "      <td>13/2/2018 2:37 am</td>\n",
       "      <td>PTCCB - MCCBS,EARTH LEAK&amp;SWITC</td>\n",
       "      <td>TEX Request for Test Report on InOperative Pro...</td>\n",
       "      <td>A Request for Test Report on InOperative Produ...</td>\n",
       "      <td>Initiate TEX</td>\n",
       "      <td>[(LC36400 breaker, ISSUE, stuck in the closed ...</td>\n",
       "      <td>2018-02-13 02:37:00</td>\n",
       "      <td>2018-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>45184334</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>3/1/2018 5:54 pm</td>\n",
       "      <td>PTCCB - MCCBS,EARTH LEAK&amp;SWITC</td>\n",
       "      <td>faulty - 10321742.   22177952</td>\n",
       "      <td>2301002465\\nLV431629 x 1 Faulty\\nFailed when t...</td>\n",
       "      <td>22177952</td>\n",
       "      <td>[(LV431629, ISSUE, faulty failed when trying t...</td>\n",
       "      <td>2018-01-03 17:54:00</td>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index Case Number Severity Priority         Country   Date/Time Opened  \\\n",
       "0      0    46181136   Normal   Normal             USA  10/2/2018 3:41 am   \n",
       "1      1    48164862   Normal   Normal             USA  24/4/2018 3:11 am   \n",
       "2      2    46224073   Normal   Normal             USA  12/2/2018 8:22 pm   \n",
       "3      3    46237499   Normal   Normal             USA  13/2/2018 2:37 am   \n",
       "4      4    45184334   Normal   Normal  United Kingdom   3/1/2018 5:54 pm   \n",
       "\n",
       "                     Product Line  \\\n",
       "0  PTCCB - MCCBS,EARTH LEAK&SWITC   \n",
       "1  PTCCB - MCCBS,EARTH LEAK&SWITC   \n",
       "2  PTCCB - MCCBS,EARTH LEAK&SWITC   \n",
       "3  PTCCB - MCCBS,EARTH LEAK&SWITC   \n",
       "4  PTCCB - MCCBS,EARTH LEAK&SWITC   \n",
       "\n",
       "                                             Subject  \\\n",
       "0                   PJA36040CU43CE1ACMOLV and EBX510   \n",
       "1              RJF36160U44A having nuisance tripping   \n",
       "2                      TEX - Square D warranty claim   \n",
       "3  TEX Request for Test Report on InOperative Pro...   \n",
       "4                      faulty - 10321742.   22177952   \n",
       "\n",
       "                                    Customer Request  \\\n",
       "0  Name: David Rosell\\nPhone: 7862264495\\nConcern...   \n",
       "1  Name: Albert Sarkis\\nPhone: (209) 744-1513\\nCo...   \n",
       "2  Please process the attached supplier recovery ...   \n",
       "3  A Request for Test Report on InOperative Produ...   \n",
       "4  2301002465\\nLV431629 x 1 Faulty\\nFailed when t...   \n",
       "\n",
       "                                  Answer To Customer  \\\n",
       "0  He had the trip unit door closed.  With it ope...   \n",
       "1  This breaker appears to have a damaged di/dt b...   \n",
       "2                                       Initiate TEX   \n",
       "3                                       Initiate TEX   \n",
       "4                                           22177952   \n",
       "\n",
       "                                            triplets            Date raw  \\\n",
       "0  [(EBX510, ISSUE, has changed the trip unit, 0.... 2018-02-10 03:41:00   \n",
       "1  [(RJF36160U44A, CONTEXT, having nuisance tripp... 2018-04-24 03:11:00   \n",
       "2                                                 [] 2018-02-12 20:22:00   \n",
       "3  [(LC36400 breaker, ISSUE, stuck in the closed ... 2018-02-13 02:37:00   \n",
       "4  [(LV431629, ISSUE, faulty failed when trying t... 2018-01-03 17:54:00   \n",
       "\n",
       "        Date  \n",
       "0 2018-02-01  \n",
       "1 2018-04-01  \n",
       "2 2018-02-01  \n",
       "3 2018-02-01  \n",
       "4 2018-01-01  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55211664",
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdf[['Case Number', 'Date', 'Severity', 'Customer Request', 'triplets']].to_csv('6_21_validation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c54432",
   "metadata": {},
   "source": [
    "#### Generate Knowledge Graph Triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 953,
   "id": "8d357e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets = np.asarray([trip for trip in rawdf['triplets'] if trip is not []])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 957,
   "id": "b2068cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_df = pd.DataFrame(flatten_list(triplets), columns=['HEAD','RELATION','TAIL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b82182c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('bFO_kg_triplet.tsv', np.asarray(triplet_df), delimiter='\\t', fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d452a29",
   "metadata": {},
   "source": [
    "### TODO\n",
    "- NER Additional Tags\n",
    "    - Product\n",
    "    - Location\n",
    "    - Contact\n",
    "    - Organization\n",
    "- **How to generate more training data**\n",
    "    - Fine-Tune T5 module on previous samples\n",
    "        - use model to generate more training data...\n",
    "    - Manually build dataset\n",
    "    - Better samples from Matcher f(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
